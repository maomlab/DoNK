#!/bin/bash

#SBATCH --job-name=
#SBATCH --account=
#SBATCH --partition=
#SBATCH --cpus-per-task=
#SBATCH --nodes=
#SBATCH --ntasks-per-node=
#SBATCH --mem-per-cpu=
#SBATCH --time=
#SBATCH --mail-type=BEGIN,END
#SBATCH --error=
#SBATCH --out=
#SBATCH --mail-user=
#SBATCH --array= ## range (1-n) where n is the number of array jobs you have (# of lines in task.txt)

# tasks.txt should contain a list of directories (one on each line). each of these directories should contain a dirlist file (generated by dirlist.sh) the output is a series of extract*.txt files

dir=$(cat tasks.txt | sed -n "$SLURM_ARRAY_TASK_ID p")

cd $dir

python $DOCKBASE/analysis/extract_all_blazing_fast.py ./dirlist extract_all.txt 20000000

